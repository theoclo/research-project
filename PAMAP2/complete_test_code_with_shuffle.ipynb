{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9cb3bf1aee3b6a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Preprocess of the PAMAP2 DataSet"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "id": "7c24a77e7feef6c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Creates overlapping windows from the input data.\n",
    "def create_windows(data, window_size, step_size):\n",
    "    windows = []\n",
    "    for i in range(0, data.shape[0] - window_size + 1, step_size):\n",
    "        windows.append(data[i:i + window_size])\n",
    "    return np.array(windows)"
   ],
   "id": "3852f22307f6e3a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "list_of_files = ['./ProtocolData/subject101.dat',\n",
    "                './ProtocolData/subject102.dat',\n",
    "                './ProtocolData/subject103.dat',\n",
    "                './ProtocolData/subject104.dat',\n",
    "                './ProtocolData/subject105.dat',\n",
    "                './ProtocolData/subject106.dat',\n",
    "                './ProtocolData/subject107.dat',\n",
    "                './ProtocolData/subject108.dat',\n",
    "                './ProtocolData/subject109.dat']"
   ],
   "id": "55319fcae83b6bc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "colNames = [\"timestamp\", \"activityID\", \"heartrate\"]\n",
    "\n",
    "IMUhand = ['handTemperature',\n",
    "            'handAcc16_1', 'handAcc16_2', 'handAcc16_3',\n",
    "            'handAcc6_1', 'handAcc6_2', 'handAcc6_3',\n",
    "            'handGyro1', 'handGyro2', 'handGyro3',\n",
    "            'handMagne1', 'handMagne2', 'handMagne3',\n",
    "            'handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4']\n",
    "\n",
    "IMUchest = ['chestTemperature',\n",
    "            'chestAcc16_1', 'chestAcc16_2', 'chestAcc16_3',\n",
    "            'chestAcc6_1', 'chestAcc6_2', 'chestAcc6_3',\n",
    "            'chestGyro1', 'chestGyro2', 'chestGyro3',\n",
    "            'chestMagne1', 'chestMagne2', 'chestMagne3',\n",
    "            'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4']\n",
    "\n",
    "IMUankle = ['ankleTemperature',\n",
    "            'ankleAcc16_1', 'ankleAcc16_2', 'ankleAcc16_3',\n",
    "            'ankleAcc6_1', 'ankleAcc6_2', 'ankleAcc6_3',\n",
    "            'ankleGyro1', 'ankleGyro2', 'ankleGyro3',\n",
    "            'ankleMagne1', 'ankleMagne2', 'ankleMagne3',\n",
    "            'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4']\n",
    "\n",
    "columns = colNames + IMUhand + IMUchest + IMUankle"
   ],
   "id": "beffc1088f31dd77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataCollection = pd.DataFrame()\n",
    "for file in list_of_files:\n",
    "    procData = pd.read_table(file, header=None, sep='\\s+')\n",
    "    procData.columns = columns\n",
    "    procData['subject_id'] = int(file[-5])\n",
    "    dataCollection = pd.concat([dataCollection, procData], ignore_index=True)\n",
    "\n",
    "dataCollection.reset_index(drop=True, inplace=True)"
   ],
   "id": "74e93c0673f3bd65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def dataCleaning(dataCollection):\n",
    "    cols_to_drop = [\n",
    "        'handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4',\n",
    "        'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4',\n",
    "        'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4'\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    cols_to_drop = [col for col in cols_to_drop if col in dataCollection.columns]\n",
    "    dataCollection = dataCollection.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "    dataCollection = dataCollection.drop(dataCollection[dataCollection.activityID == 0].index)\n",
    "    dataCollection = dataCollection.apply(pd.to_numeric, errors='coerce')\n",
    "    dataCollection = dataCollection.interpolate()\n",
    "\n",
    "\n",
    "    # Keep only the desired activity IDs\n",
    "    valid_activity_ids = [1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24]\n",
    "    dataCollection = dataCollection[dataCollection['activityID'].isin(valid_activity_ids)]\n",
    "\n",
    "    # Remap activity IDs to make it go from 0 to 11\n",
    "    activity_id_mapping = {\n",
    "        1:0,\n",
    "        2:1,\n",
    "        3:2,\n",
    "        4:3,\n",
    "        5:4,\n",
    "        6:5,\n",
    "        7:6,\n",
    "        12: 7,\n",
    "        13: 8,\n",
    "        16: 9,\n",
    "        17: 10,\n",
    "        24: 11\n",
    "    }\n",
    "    dataCollection['activityID'] = dataCollection['activityID'].replace(activity_id_mapping).astype(int)\n",
    "\n",
    "    return dataCollection"
   ],
   "id": "95ae8a5933db0e88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataCol = dataCleaning(dataCollection)\n",
    "\n",
    "dataCol.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for i in range(0,4):\n",
    "    dataCol.loc[i,\"heartrate\"]=100\n",
    "\n",
    "X = dataCol.drop(['activityID', \"timestamp\", \"subject_id\"], axis=1)\n",
    "y = dataCol['activityID']"
   ],
   "id": "5d6850e85f7ff7c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ],
   "id": "9fc791af7a639c1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply time windowing\n",
    "window_size = 128\n",
    "step_size = 41\n",
    "X_windowed = create_windows(X_scaled, window_size, step_size)"
   ],
   "id": "b359e6c2d54a9390",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get corresponding labels for each window\n",
    "y_windowed = []\n",
    "for i in range(0, X_scaled.shape[0] - window_size + 1, step_size):\n",
    "    # Use the label of the majority of the activities within the window.\n",
    "    window_labels = y[i:i + window_size]\n",
    "    y_windowed.append(int(window_labels.mode()[0]))\n",
    "y_windowed = np.array(y_windowed)"
   ],
   "id": "cc3f39f9d92ed826",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split data into training and testing sets (85% train, 15% test)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X_windowed, y_windowed, test_size=0.15, random_state=42, stratify=y_windowed)"
   ],
   "id": "727f9dba3438e7d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Further split the training data into training and validation sets (82% train, 18% validation of the original training set)\n",
    "train_X_new, val_X, train_y_new, val_y = train_test_split(train_X, train_y, test_size=(1-70/85), random_state=42, stratify=train_y)"
   ],
   "id": "1623091d5342126c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"train x shape:\",train_X_new.shape)\n",
    "print(\"train y shape:\",train_y_new.shape)\n",
    "print(\"test x shape:\",test_X.shape)\n",
    "print(\"test y shape:\",test_y.shape)\n",
    "print(\"validation x shape:\",val_X.shape)\n",
    "print(\"validation y shape:\",val_y.shape)"
   ],
   "id": "d6f8673f0059292d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c0ec728e81ee1fa",
   "metadata": {},
   "source": [
    "# Save the data as numpy arrays with the specified variable names\n",
    "np.save('train_X_new_shuffled.npy', train_X_new)\n",
    "np.save('train_y_new_shuffled.npy', train_y_new)\n",
    "np.save('test_X_shuffled.npy', test_X)\n",
    "np.save('test_Y_shuffled.npy', test_y)\n",
    "np.save('val_X_shuffled.npy', val_X)\n",
    "np.save('val_Y_shuffled.npy', val_y)\n",
    "\n",
    "print(\"Data successfully saved as NumPy arrays: train_X_new_shuffled.npy, train_y_new_shuffled.npy, test_X_shuffled.npy, test_Y_shuffled.npy, val_X_shuffled.npy, val_Y_shuffled.npy\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "261cbc3d31878a3b",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "id": "60d94da8642de368",
   "metadata": {},
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from sklearn.metrics import recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import os\n",
    "import gc\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b43bef2d6401ee27",
   "metadata": {},
   "source": [
    "# Enable PyTorch memory optimization features\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'\n",
    "\n",
    "gc.collect()  # Force garbage collection before starting"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "777f5cc502b4b24b",
   "metadata": {},
   "source": [
    "torch.cuda.set_device(0)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(n_gpu)\n",
    "path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "print(path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb3f94573e353763",
   "metadata": {},
   "source": [
    "pathlist = ['./train_X_new_shuffled.npy',\n",
    "            './train_y_new_shuffled.npy',\n",
    "            './val_X_shuffled.npy',\n",
    "            './val_Y_shuffled.npy',\n",
    "            './test_X_shuffled.npy',\n",
    "            './test_Y_shuffled.npy']\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13275d2c944f513b",
   "metadata": {},
   "source": [
    "def similarity_matrix(x, chunk_size=32):\n",
    "    ''' Calculate adjusted cosine similarity matrix of size x.size(0) x x.size(0) with memory optimization. '''\n",
    "    # Free memory before computation\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Process input tensor based on dimensions\n",
    "    if x.dim() == 4:\n",
    "        if x.size(1) > 3 and x.size(2) > 1:\n",
    "            z = x.view(x.size(0), x.size(1), -1)\n",
    "            x = z.std(dim=2)\n",
    "        else:\n",
    "            x = x.view(x.size(0), -1)\n",
    "\n",
    "    # Center the data\n",
    "    xc = x - x.mean(dim=1).unsqueeze(1)\n",
    "\n",
    "    # Normalize\n",
    "    xn = xc / (1e-8 + torch.sqrt(torch.sum(xc ** 2, dim=1))).unsqueeze(1)\n",
    "\n",
    "    # Use chunking for large matrices to reduce memory usage\n",
    "    batch_size = xn.size(0)\n",
    "\n",
    "    # If batch is small enough, compute directly\n",
    "    if batch_size <= chunk_size:\n",
    "        R = xn.matmul(xn.transpose(1, 0)).clamp(-1, 1)\n",
    "        return R\n",
    "\n",
    "    # For larger batches, compute similarity matrix in chunks\n",
    "    R = torch.zeros(batch_size, batch_size, device=xn.device)\n",
    "\n",
    "    for i in range(0, batch_size, chunk_size):\n",
    "        end_i = min(i + chunk_size, batch_size)\n",
    "        chunk_i = xn[i:end_i]\n",
    "\n",
    "        for j in range(0, batch_size, chunk_size):\n",
    "            end_j = min(j + chunk_size, batch_size)\n",
    "            chunk_j = xn[j:end_j]\n",
    "\n",
    "            # Compute similarity for this chunk\n",
    "            R[i:end_i, j:end_j] = chunk_i.matmul(chunk_j.transpose(1, 0)).clamp(-1, 1)\n",
    "\n",
    "            # Free memory after each chunk computation\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return R\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23353edbbc7be6c9",
   "metadata": {},
   "source": [
    "def data_flat(data_y):\n",
    "    # Check if data_y is 1D or multi-dimensional\n",
    "    if data_y.ndim == 1:\n",
    "        # If 1D, return as is\n",
    "        return data_y\n",
    "    else:\n",
    "        # If multi-dimensional, use argmax along axis 1\n",
    "        return np.argmax(data_y, axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3295691722d73716",
   "metadata": {},
   "source": [
    "def load_data(train_x_path, train_y_path, batchsize):\n",
    "    # Clear GPU memory before loading data\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Load the data using memory mapping to avoid loading entire array into memory\n",
    "    train_x_mmap = np.load(train_x_path, mmap_mode='r')\n",
    "    train_y_mmap = np.load(train_y_path, mmap_mode='r')\n",
    "\n",
    "    # Get original data size\n",
    "    original_size = train_x_mmap.shape[0]\n",
    "\n",
    "    # Use only 1/2 of the data to reduce memory usage\n",
    "    subset_size = original_size\n",
    "\n",
    "    # Get data dimensions\n",
    "    if len(train_x_mmap.shape) == 3:\n",
    "        channels = 1\n",
    "        height = train_x_mmap.shape[1]\n",
    "        width = train_x_mmap.shape[2]\n",
    "    else:\n",
    "        channels = train_x_mmap.shape[1]\n",
    "        height = train_x_mmap.shape[2]\n",
    "        width = train_x_mmap.shape[3]\n",
    "\n",
    "    # Display the size of the data\n",
    "    print('Original data size:', original_size)\n",
    "    print('Using all of data:', subset_size)\n",
    "    print('Data shape after reduction:', (subset_size, channels, height, width))\n",
    "\n",
    "    # Create a custom dataset that loads data in chunks\n",
    "    class MemoryEfficientDataset(Data.Dataset):\n",
    "        def __init__(self, x_mmap, y_mmap, subset_size):\n",
    "            self.x_mmap = x_mmap\n",
    "            self.y_mmap = y_mmap\n",
    "            self.subset_size = min(subset_size, x_mmap.shape[0])\n",
    "\n",
    "            # Calculate class distribution\n",
    "            if y_mmap.ndim == 1:\n",
    "                # Sample a small portion to get class distribution\n",
    "                sample_size = min(1000, self.subset_size)\n",
    "                sample_indices = np.random.choice(self.subset_size, sample_size, replace=False)\n",
    "                sample_y = np.array(y_mmap[sample_indices])\n",
    "                unique_classes, counts = np.unique(sample_y, return_counts=True)\n",
    "            else:\n",
    "                sample_size = min(1000, self.subset_size)\n",
    "                sample_indices = np.random.choice(self.subset_size, sample_size, replace=False)\n",
    "                sample_y = np.array(y_mmap[sample_indices])\n",
    "                unique_classes, counts = np.unique(np.argmax(sample_y, axis=1), return_counts=True)\n",
    "\n",
    "            print('\\nDistribution of activities in training data (sampled):')\n",
    "            for cls, count in zip(unique_classes, counts):\n",
    "                print(f'Activity {cls}: {count} samples ({count/sample_size*100:.2f}%)')\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.subset_size\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # Load single sample on demand\n",
    "            if idx < self.subset_size:\n",
    "                # Convert to float32 to reduce memory usage\n",
    "                x = np.array(self.x_mmap[idx], dtype=np.float32)\n",
    "                y = np.array(self.y_mmap[idx])\n",
    "\n",
    "                # Reshape if needed\n",
    "                if len(x.shape) == 2:\n",
    "                    x = x.reshape(1, x.shape[0], x.shape[1])\n",
    "\n",
    "                # Convert to torch tensors\n",
    "                x_tensor = torch.from_numpy(x).float()\n",
    "                y_tensor = torch.from_numpy(y)\n",
    "\n",
    "                return x_tensor, y_tensor\n",
    "            else:\n",
    "                raise IndexError(\"Index out of bounds\")\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = MemoryEfficientDataset(train_x_mmap, train_y_mmap, subset_size)\n",
    "\n",
    "    # Use a DataLoader with pin_memory for faster GPU transfer\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batchsize,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,  # This speeds up the host to GPU transfer\n",
    "    )\n",
    "\n",
    "    # Print shape information\n",
    "    print('^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n',\n",
    "          f'Batches: {len(loader)}, Batch size: {batchsize}, Total samples: {len(dataset)}',\n",
    "          '\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^')\n",
    "\n",
    "    return loader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c78e20c722cd669e",
   "metadata": {},
   "source": [
    "def quzheng_x(height, kernel_size, padding, stride, numlayer):\n",
    "    list = []\n",
    "    for i in range(1, numlayer + 1):\n",
    "        feature = int((height - kernel_size + 2 * padding) / stride) + 1\n",
    "        height = feature\n",
    "        list.append(feature)\n",
    "    return list"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2fae59a5ce914ebc",
   "metadata": {},
   "source": [
    "def quzheng_s(height, kernel_size, padding, stride, numlayer):\n",
    "    list = []\n",
    "    for i in range(1, numlayer + 1):\n",
    "        torch.cuda.empty_cache()\n",
    "        feature = math.ceil((height - kernel_size + 2 * padding) / stride) + 1\n",
    "        height = feature\n",
    "        list.append(feature)\n",
    "        torch.cuda.empty_cache()\n",
    "    return list"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f5f4a8770ca6697",
   "metadata": {},
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Loss, self).__init__()\n",
    "        print('this is loss function!')\n",
    "\n",
    "    def forward(self, output, label):\n",
    "        loss_func = F.cross_entropy(output, label)\n",
    "        return loss_func"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b14a6629dcd3829f",
   "metadata": {},
   "source": [
    "class conv_loss_block(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out, height_width, kernel, stride, bias,numlayer):\n",
    "        super(conv_loss_block, self).__init__()\n",
    "        self.channel_in = channel_in\n",
    "        self.channel_out = channel_out\n",
    "        self.height_width = height_width\n",
    "        self.num_class = 12\n",
    "        self.bias = bias\n",
    "        self.dropout_p = 0.3 #avant à 5\n",
    "        self.batchnorm = True\n",
    "        self.decode_ys=[]\n",
    "        self.bns_decode_ys = []\n",
    "\n",
    "        decode_t_list = [204672, 116736, 42624]\n",
    "        # print(int(channel_out*height_width*0.5),'self.biasself.biasself.biasself.biasself.bias')\n",
    "        print(channel_in, channel_out, height_width, kernel, stride, bias,'channel_in, channel_out, height_width, kernel, stride, bias')\n",
    "\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(channel_in, channel_out, (6,2), stride=stride, padding=0, bias=self.bias),\n",
    "            nn.BatchNorm2d(channel_out),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "        for i in range(3):\n",
    "            decode_y = nn.Linear(decode_t_list[i], 12)\n",
    "            setattr(self, 'decode_y%i' % i, decode_y)\n",
    "            self._set_init(decode_y)\n",
    "            self.decode_ys.append(decode_y)\n",
    "\n",
    "\n",
    "\n",
    "        self.conv_loss = nn.Sequential(\n",
    "            nn.Conv2d(channel_out, channel_out, kernel_size=(2,1), stride=(2,1), padding=(1,0), bias=False),\n",
    "            # nn.BatchNorm2d(channel_out,momentum=0.5)\n",
    "                                       )\n",
    "\n",
    "\n",
    "        if self.batchnorm:\n",
    "            self.bn = torch.nn.BatchNorm2d(channel_out, momentum=0.5)\n",
    "            nn.init.constant_(self.bn.weight, 1)\n",
    "            nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "        self.nonlin = nn.ReLU(inplace=True)\n",
    "        self.dropout = torch.nn.Dropout(p=self.dropout_p, inplace=False)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "        self.clear_stats()\n",
    "    def _set_init(self, layer):\n",
    "        init.normal_(layer.weight, mean=0., std=.1)\n",
    "        init.constant_(layer.bias, 0.2)\n",
    "\n",
    "    def clear_stats(self):\n",
    "        self.loss_sim = 0.0\n",
    "        self.loss_pred = 0.0\n",
    "        self.correct = 0\n",
    "        self.examples = 0\n",
    "\n",
    "    def print_stats(self):\n",
    "        stats = '{}, loss_sim={:.4f}, loss_pred={:.4f}, error={:.3f}%, num_examples={}\\n'.format(\n",
    "            self.encoder,\n",
    "            self.loss_sim / self.examples,\n",
    "            self.loss_pred / self.examples,\n",
    "            100.0 * float(self.examples - self.correct) / self.examples,\n",
    "            self.examples)\n",
    "        return stats\n",
    "\n",
    "    def set_learning_rate(self, lr):\n",
    "        self.lr = lr\n",
    "        # print('lr:', self.optimizer.param_groups[0]['lr'])\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.lr\n",
    "\n",
    "    def optim_zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def optim_step(self):\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def forward(self, x, y, y_onehot, loop, is_training):\n",
    "        # Free memory before computation\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Encode input\n",
    "        h = self.encoder(x)\n",
    "\n",
    "        h_return = h\n",
    "        h_return = self.dropout(h_return)\n",
    "\n",
    "        # Apply convolution for loss calculation\n",
    "        h_loss = self.conv_loss(h)\n",
    "\n",
    "        # Calculate similarity matrices with memory-efficient implementation\n",
    "        # Use smaller chunk size for validation to reduce memory usage\n",
    "        chunk_size = 16 if not is_training else 32\n",
    "\n",
    "        # Calculate similarity matrix for h_loss\n",
    "        Rh = similarity_matrix(h_loss, chunk_size=chunk_size)\n",
    "\n",
    "        # Calculate loss components\n",
    "        h_pool = h_return\n",
    "\n",
    "        # Flatten and pass through decoder\n",
    "        h_pool_flat = h_pool.view(h_pool.size(0), -1)\n",
    "        y_hat_local = self.decode_ys[loop](h_pool_flat)\n",
    "\n",
    "        # Calculate prediction loss\n",
    "        loss_pred = (1 - 0.99) * F.cross_entropy(y_hat_local, y.detach().long())\n",
    "\n",
    "        # Calculate similarity loss\n",
    "        if is_training:\n",
    "            # During training, use full similarity calculation\n",
    "            Ry = similarity_matrix(y_onehot, chunk_size=chunk_size).detach()\n",
    "            loss_sim = 0.99 * F.mse_loss(Rh, Ry)\n",
    "\n",
    "            # Calculate unsupervised loss only during training\n",
    "            Rx = similarity_matrix(x, chunk_size=chunk_size).detach()\n",
    "            loss_unsup = F.mse_loss(Rh, Rx)\n",
    "        else:\n",
    "            # During validation, use a simplified approach to save memory\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                Ry = similarity_matrix(y_onehot, chunk_size=chunk_size).detach()\n",
    "                loss_sim = 0.99 * F.mse_loss(Rh, Ry)\n",
    "                loss_unsup = 0.0  # Skip unsupervised loss during validation\n",
    "\n",
    "        # Combine losses\n",
    "        loss_sup = loss_pred + loss_sim\n",
    "        loss = loss_sup * 1 + loss_unsup * 0\n",
    "\n",
    "        # Backpropagation (only during training)\n",
    "        if is_training:\n",
    "            loss.backward(retain_graph=False)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            h_return.detach_()\n",
    "\n",
    "        # Free memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Return loss as scalar\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        return h_return, loss_value"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13d4bb225c9a03ef",
   "metadata": {},
   "source": [
    "class convnet(nn.Module):\n",
    "    def __init__(self, input_ch, output_ch, height, num_layers, num_hiden, num_classes,lr):\n",
    "        super(convnet, self).__init__()\n",
    "        self.num_hidden = num_hiden\n",
    "        self.num_layers = num_layers\n",
    "        self.height = height\n",
    "        self.input_ch = input_ch\n",
    "        self.output_ch = output_ch\n",
    "        reduce_factor = 1\n",
    "        self.bn=[]\n",
    "\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [conv_loss_block(self.input_ch, self.output_ch, self.height, kernel=(6,2), stride=(3,1), bias=False,numlayer=num_layers)])\n",
    "        self.layers.extend(\n",
    "            [conv_loss_block(self.output_ch, 256, self.height, kernel=(6,2), stride=(3,1), bias=False,numlayer=num_layers),\n",
    "             conv_loss_block(256, 384, self.height, kernel=(6,2), stride=(3,1), bias=False,numlayer=num_layers)\n",
    "             ])\n",
    "\n",
    "        self.layer_out = nn.Linear(42624, num_classes)\n",
    "        self.layer_out.weight.data.zero_()\n",
    "\n",
    "        bn = nn.BatchNorm2d(1, momentum=0.5)\n",
    "        setattr(self, 'pre_bn' , bn)\n",
    "        self.bn.append(bn)\n",
    "\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.layer_out.parameters()\n",
    "\n",
    "    def set_learning_rate(self, lr):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.set_learning_rate(lr)\n",
    "\n",
    "    def optim_step(self):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.optim_step()\n",
    "\n",
    "    def optim_zero_grad(self):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.optim_zero_grad()\n",
    "\n",
    "    def forward(self, x, y, y_onehot, is_training):\n",
    "        # Clear GPU memory before starting\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Initialize total loss\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Process through each layer\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # Clear GPU memory before each layer\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Apply batch normalization to first layer input\n",
    "            if i == 0:\n",
    "                # Ensure input is float32 before batch normalization\n",
    "                x = x.float() if not x.is_floating_point() else x\n",
    "                x = self.bn[i](x)\n",
    "\n",
    "            # Use mixed precision for validation\n",
    "            if not is_training:\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    x, loss = layer(x, y, y_onehot, i, is_training)\n",
    "            else:\n",
    "                x, loss = layer(x, y, y_onehot, i, is_training)\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_loss += loss\n",
    "\n",
    "            # Clear GPU memory after each layer\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Final output layer\n",
    "        # Make sure tensor is contiguous in memory for efficient reshape\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "\n",
    "        # Apply output layer\n",
    "        if not is_training:\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                x = self.layer_out(x)\n",
    "        else:\n",
    "            x = self.layer_out(x)\n",
    "\n",
    "        # Final memory cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return x, total_loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "60dd29759bc2a2cb",
   "metadata": {},
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    ''' Take integer tensor y with n dims and convert it to 1-hot representation with n+1 dims. '''\n",
    "    # Handle the case where y might be on GPU\n",
    "    if y.is_cuda:\n",
    "        y_cpu = y.cpu()\n",
    "    else:\n",
    "        y_cpu = y\n",
    "\n",
    "    # Convert to LongTensor and reshape to column vector\n",
    "    y_tensor = y_cpu.type(torch.LongTensor).view(-1, 1)\n",
    "\n",
    "    # Determine number of dimensions (classes)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "\n",
    "    # Create one-hot encoding\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "\n",
    "    # Reshape back to original shape plus one-hot dimension\n",
    "    if len(y.shape) == 1:\n",
    "        # If y is 1D, keep y_one_hot as 2D (samples x classes)\n",
    "        pass\n",
    "    else:\n",
    "        # Otherwise reshape to match original dimensions plus one-hot\n",
    "        y_one_hot = y_one_hot.view(*y.shape, -1)\n",
    "\n",
    "    # Move back to GPU if original tensor was on GPU\n",
    "    if y.is_cuda:\n",
    "        y_one_hot = y_one_hot.cuda()\n",
    "\n",
    "    return y_one_hot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e20eeb945fda908a",
   "metadata": {},
   "source": [
    "def plot_confusion(cm, class_data, title='Confusion Matrix',save_title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    classes = class_data\n",
    "    im = plt.imshow(cm, interpolation='none', cmap=plt.cm.Oranges)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(tick_marks, classes, fontsize=10)\n",
    "    plt.ylabel('True label', fontsize=12)\n",
    "    plt.xlabel('Predicted label', fontsize=12)\n",
    "    plt.tick_params(labelsize=10)\n",
    "    plt.grid(False)  # Supprime les lignes de grille\n",
    "\n",
    "    # Affichage des valeurs réelles dans chaque case, police petite, centré\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, f\"{cm[i, j]}\", ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_title, format='pdf')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f6ed4d184bb837ab",
   "metadata": {},
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Create gradient scaler for mixed precision training\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "def train(train_loader, val_error):\n",
    "    # Clear GPU memory before training\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Calculate and display model parameters\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('Total_Number of params: {} |Trainable_num of params: {}'.format(total_num, trainable_num))\n",
    "\n",
    "    # Gradient accumulation steps to simulate larger batch size\n",
    "    accumulation_steps = 4\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Track memory usage and training accuracy\n",
    "    peak_memory = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Iterate through batches\n",
    "    for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        # Move data to GPU just before use\n",
    "        batch_x = batch_x.cuda(non_blocking=True)  # non_blocking=True improves performance\n",
    "        batch_y = batch_y.cuda(non_blocking=True)\n",
    "\n",
    "        # Convert targets to one-hot encoding\n",
    "        with torch.no_grad():  # No need to track gradients for one-hot conversion\n",
    "            target_onehot = to_one_hot(batch_y).cuda()\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            # Use mixed precision for forward and backward pass\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                # Forward pass\n",
    "                output, _ = model(batch_x, batch_y, target_onehot, True)\n",
    "                loss = l(output, batch_y.long())\n",
    "                # Scale loss for gradient accumulation\n",
    "                loss = loss / accumulation_steps\n",
    "\n",
    "                # Calculate training accuracy\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y.long()).sum().item()\n",
    "\n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Step optimizer after accumulation steps\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                # Unscale gradients for any gradient clipping (optional)\n",
    "                # scaler.unscale_(optimizer)\n",
    "                # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "                # Update weights and scaler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Track peak memory usage\n",
    "            current_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)  # Convert to MB\n",
    "            peak_memory = max(peak_memory, current_memory)\n",
    "\n",
    "            # Free memory\n",
    "            del batch_x, batch_y, target_onehot, output, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(f\"OOM at training step {step}, trying to recover...\")\n",
    "                # Free memory\n",
    "                if 'batch_x' in locals(): del batch_x\n",
    "                if 'batch_y' in locals(): del batch_y\n",
    "                if 'target_onehot' in locals(): del target_onehot\n",
    "                if 'output' in locals(): del output\n",
    "                if 'loss' in locals(): del loss\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    # Print peak memory usage and training accuracy\n",
    "    print(f\"Peak memory usage during training: {peak_memory:.2f} MB\")\n",
    "    train_accuracy = 100 * correct / total if total > 0 else 0\n",
    "    print(f\"Training accuracy: {train_accuracy:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb0f628a1683aa6d",
   "metadata": {},
   "source": [
    "def validate(test_x_path, test_y_path, test_error, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    # Clear GPU memory before validation\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Set batch size for validation - smaller batches use less memory\n",
    "    batch_size = 32\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Load test data using memory mapping\n",
    "        test_x_mmap = np.load(test_x_path, mmap_mode='r')\n",
    "        test_y_mmap = np.load(test_y_path, mmap_mode='r')\n",
    "\n",
    "        # Get test data dimensions\n",
    "        test_x_shape = test_x_mmap.shape\n",
    "        test_size = test_x_shape[0]\n",
    "\n",
    "        # Initialize arrays to store predictions and targets\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "\n",
    "        # Process data in batches to reduce memory usage\n",
    "        for start_idx in range(0, test_size, batch_size):\n",
    "            # Clear GPU memory before processing each batch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Get batch indices\n",
    "            end_idx = min(start_idx + batch_size, test_size)\n",
    "\n",
    "            # Load batch data\n",
    "            batch_x_array = np.array(test_x_mmap[start_idx:end_idx], dtype=np.float32)\n",
    "            batch_y_array = np.array(test_y_mmap[start_idx:end_idx])\n",
    "\n",
    "            # Reshape and convert to tensors\n",
    "            batch_x = torch.from_numpy(\n",
    "                np.reshape(batch_x_array, [end_idx - start_idx, 1, test_x_shape[1], test_x_shape[2]])\n",
    "            ).half().cuda()  # Use half precision to save memory\n",
    "\n",
    "            batch_y = torch.from_numpy(batch_y_array).cuda()\n",
    "            batch_y_onehot = to_one_hot(batch_y).cuda()\n",
    "\n",
    "            # Free memory\n",
    "            del batch_x_array\n",
    "            del batch_y_array\n",
    "\n",
    "            try:\n",
    "                # Use mixed precision for inference\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    batch_output, _ = model(batch_x, batch_y, batch_y_onehot, False)\n",
    "\n",
    "                # Store predictions and targets for later evaluation\n",
    "                all_predictions.append(batch_output.cpu().detach())\n",
    "                all_targets.append(batch_y.cpu().detach())\n",
    "\n",
    "                # Free memory\n",
    "                del batch_x\n",
    "                del batch_y\n",
    "                del batch_y_onehot\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                # If we still get OOM error, try with even smaller batch\n",
    "                if \"CUDA out of memory\" in str(e):\n",
    "                    print(f\"OOM at batch {start_idx}-{end_idx}, trying with smaller batch\")\n",
    "                    # Free all GPU memory\n",
    "                    del batch_x\n",
    "                    del batch_y\n",
    "                    del batch_y_onehot\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    # Try again with half the batch size\n",
    "                    half_batch = (end_idx - start_idx) // 2\n",
    "                    if half_batch > 0:\n",
    "                        for half_start in range(start_idx, end_idx, half_batch):\n",
    "                            half_end = min(half_start + half_batch, end_idx)\n",
    "\n",
    "                            # Load smaller batch\n",
    "                            small_batch_x = np.array(test_x_mmap[half_start:half_end], dtype=np.float32)\n",
    "                            small_batch_y = np.array(test_y_mmap[half_start:half_end])\n",
    "\n",
    "                            # Reshape and convert to tensors\n",
    "                            small_batch_x = torch.from_numpy(\n",
    "                                np.reshape(small_batch_x, [half_end - half_start, 1, test_x_shape[1], test_x_shape[2]])\n",
    "                            ).half().cuda()\n",
    "\n",
    "                            small_batch_y = torch.from_numpy(small_batch_y).cuda()\n",
    "                            small_batch_y_onehot = to_one_hot(small_batch_y).cuda()\n",
    "\n",
    "                            # Process smaller batch\n",
    "                            with torch.amp.autocast('cuda'):\n",
    "                                small_batch_output, _ = model(small_batch_x, small_batch_y, small_batch_y_onehot, False)\n",
    "\n",
    "                            # Store results\n",
    "                            all_predictions.append(small_batch_output.cpu().detach())\n",
    "                            all_targets.append(small_batch_y.cpu().detach())\n",
    "\n",
    "                            # Free memory\n",
    "                            del small_batch_x\n",
    "                            del small_batch_y\n",
    "                            del small_batch_y_onehot\n",
    "                            torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    # Re-raise if it's not an OOM error\n",
    "                    raise\n",
    "\n",
    "        # Free memory-mapped arrays\n",
    "        del test_x_mmap\n",
    "        del test_y_mmap\n",
    "\n",
    "        # Combine results from all batches\n",
    "        try:\n",
    "            all_pred_tensor = torch.cat(all_predictions, dim=0)\n",
    "            all_target_tensor = torch.cat(all_targets, dim=0)\n",
    "\n",
    "            # Convert to numpy for metric calculation\n",
    "            all_pred_numpy = data_flat(all_pred_tensor.numpy())\n",
    "            all_target_numpy = all_target_tensor.numpy()\n",
    "\n",
    "            # Calculate metrics using class labels (not one-hot encoded)\n",
    "            acc = accuracy_score(all_target_numpy, all_pred_numpy)\n",
    "            f1 = f1_score(all_target_numpy, all_pred_numpy, average='weighted')\n",
    "            f2 = f1_score(all_target_numpy, all_pred_numpy, average='micro')\n",
    "            f3 = f1_score(all_target_numpy, all_pred_numpy, average='macro')\n",
    "            reca = recall_score(all_target_numpy, all_pred_numpy, average='weighted')\n",
    "\n",
    "\n",
    "            # Print detailed epoch information\n",
    "            print('Epoch: ', epoch, '| validation accuracy: %.8f' % acc, '| validation F1: %.8f' % f1,\n",
    "                  '| validation recall: %.8f' % reca, '| validation micro: %.8f' % f2, '| validation macro: %.8f' % f3)\n",
    "\n",
    "            # Alternative accuracy calculation\n",
    "            pred_classes = torch.max(all_pred_tensor, 1)[1]\n",
    "            accuracy = (torch.sum(pred_classes == all_target_tensor.long()).float() / all_target_tensor.size(0))\n",
    "\n",
    "            # Store error for plotting if needed\n",
    "            test_error.append((1 - accuracy.item()))\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Error in metric calculation: {e}\")\n",
    "            # Fallback to simpler accuracy calculation\n",
    "            pred_classes = torch.max(all_pred_tensor, 1)[1]\n",
    "            accuracy = (torch.sum(pred_classes == all_target_tensor.long()).float() / all_target_tensor.size(0))\n",
    "            print('Epoch: ', epoch, '| validation accuracy: %.8f' % accuracy.item())\n",
    "            test_error.append((1 - accuracy.item()))\n",
    "\n",
    "        # Free memory\n",
    "        del all_predictions\n",
    "        del all_targets\n",
    "        if 'all_pred_tensor' in locals(): del all_pred_tensor\n",
    "        if 'all_target_tensor' in locals(): del all_target_tensor\n",
    "\n",
    "    # Switch back to training mode\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec8253092dfbf88d",
   "metadata": {},
   "source": [
    "import time\n",
    "from river import metrics\n",
    "from river import stream\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ab1c28d381d65ab",
   "metadata": {},
   "source": [
    "# Define the OASW method using CNN instead of LightGBM\n",
    "def oasw(model, optimizer, loss_fn, test_loader, a=0.99, b=0.95, win1=200, win2=1000):\n",
    "\n",
    "    model.eval()\n",
    "    metric = metrics.Accuracy()\n",
    "\n",
    "    a = round(a, 3)\n",
    "    b = round(b, 3)\n",
    "    win1 = round(win1)\n",
    "    win2 = round(win2)\n",
    "\n",
    "    i = 0\n",
    "    yt = []\n",
    "    yp = []\n",
    "    x_new = []\n",
    "    y_new = []\n",
    "    dr = []\n",
    "    d = 0\n",
    "    f = 0\n",
    "    tt = 0\n",
    "    th = 0\n",
    "    xt = []\n",
    "    t = []\n",
    "    m = []\n",
    "    f1s=[]\n",
    "    mems=[]\n",
    "\n",
    "\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        xi2_tensor = batch_x.cuda()\n",
    "        yi_tensor = batch_y.cuda()\n",
    "        yi_onehot = to_one_hot(yi_tensor).cuda()\n",
    "\n",
    "        # Prédiction\n",
    "        with torch.no_grad():\n",
    "            output, _ = model(xi2_tensor, yi_tensor, yi_onehot, False)\n",
    "            _, y_pred = torch.max(output.data, 1)\n",
    "            y_pred_scalar = y_pred.item()\n",
    "\n",
    "        yi = batch_y.item()\n",
    "        xi2 = batch_x.cpu().numpy()\n",
    "\n",
    "        # Update the metric\n",
    "        metric.update(int(yi), y_pred_scalar)\n",
    "\n",
    "        # Store the y_test, y_pred, and x_test\n",
    "        yt.append(int(yi))\n",
    "        yp.append(y_pred_scalar)\n",
    "        xt.append(xi2[0])\n",
    "\n",
    "        t.append(i)\n",
    "        acc_current = metric.get()*100\n",
    "        m.append(acc_current)\n",
    "\n",
    "        # F1 score cumulatif\n",
    "        f1s.append(f1_score(yt, yp, average='weighted'))\n",
    "        # Mémoire GPU utilisée (en MB)\n",
    "        mems.append(torch.cuda.max_memory_allocated() / (1024 ** 2))\n",
    "\n",
    "        # Monitor the accuracy changes in the sliding window\n",
    "        if i > 2 * win1:\n",
    "            acc1 = accuracy_score(yt[i - win1:], yp[i - win1:])  # Current window accuracy\n",
    "            acc2 = accuracy_score(yt[i - 2 * win1:i - win1], yp[i - 2 * win1:i - win1])  # Last window accuracy\n",
    "            if (d == 0) & (acc1 < a * acc2):  # If the window accuracy drops to the warning level\n",
    "                x_new.append(xi2[0])\n",
    "                y_new.append(int(yi))\n",
    "                d = 1\n",
    "                print(\"Warning level\")\n",
    "            if d == 1:  # In the warning level\n",
    "                tt = len(y_new)\n",
    "                if acc1 < b * acc2:  # If the window accuracy drops to the drift level\n",
    "                    dr.append(i)  # Record the drift start point\n",
    "                    f = i\n",
    "                    if tt < win1:  # if enough new concept samples are collected\n",
    "                        # Retrain with recent samples\n",
    "                        model.train()\n",
    "                        # Convert xt and yt to tensors for training\n",
    "                        recent_x = np.array(xt[i - win1:])\n",
    "                        recent_y = np.array(yt[i - win1:])\n",
    "                        recent_x_tensor = torch.from_numpy(recent_x).float().cuda()\n",
    "                        recent_y_tensor = torch.from_numpy(recent_y).long().cuda()\n",
    "                        recent_y_onehot = to_one_hot(recent_y_tensor).cuda()\n",
    "\n",
    "                        print(\"debut train warning1\")\n",
    "                        # Train for a few iterations\n",
    "                        for _ in range(10):\n",
    "                            optimizer.zero_grad()\n",
    "                            output, _ = model(recent_x_tensor, recent_y_tensor, recent_y_onehot, True)\n",
    "                            loss = loss_fn(output, recent_y_tensor)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                        print(\"fin train warning1\")\n",
    "                    else:\n",
    "                        # Retrain with new concept samples\n",
    "                        model.train()\n",
    "                        # Convert x_new and y_new to tensors for training\n",
    "                        new_x = np.array(x_new)\n",
    "                        new_y = np.array(y_new)\n",
    "                        new_x_tensor = torch.from_numpy(new_x).float().cuda()\n",
    "                        new_y_tensor = torch.from_numpy(new_y).long().cuda()\n",
    "                        new_y_onehot = to_one_hot(new_y_tensor).cuda()\n",
    "\n",
    "                        # Train for a few iterations\n",
    "                        print(\"debut train warning2\")\n",
    "                        model.train()\n",
    "                        for _ in range(10):\n",
    "                            optimizer.zero_grad()\n",
    "                            output, _ = model(new_x_tensor, new_y_tensor, new_y_onehot, True)\n",
    "                            loss = loss_fn(output, new_y_tensor)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                        model.eval()\n",
    "                        print(\"fin train warning2\")\n",
    "                    d = 2\n",
    "                    print(\"Drift level\")\n",
    "                elif (acc1 > a * acc2) | (tt == win2):  # If the window accuracy increases back to normal\n",
    "                    x_new = []\n",
    "                    y_new = []\n",
    "                    d = 0\n",
    "                else:\n",
    "                    x_new.append(xi2[0])\n",
    "                    y_new.append(int(yi))\n",
    "            if d == 2:  # In the drift level\n",
    "                tt = len(y_new)\n",
    "                acc3 = accuracy_score(yt[f:f + win1], yp[f:f + win1])\n",
    "                x_new.append(xi2[0])\n",
    "                y_new.append(int(yi))\n",
    "                if tt >= win1:\n",
    "                    if acc1 < a * acc3:  # When new concept accuracy drops to the warning level\n",
    "                        if th == 0:\n",
    "                            # Retrain with new concept samples\n",
    "                            model.train()\n",
    "                            # Convert x_new and y_new to tensors for training\n",
    "                            new_x = np.array(x_new)\n",
    "                            new_y = np.array(y_new)\n",
    "                            new_x_tensor = torch.from_numpy(new_x).float().cuda()\n",
    "                            new_y_tensor = torch.from_numpy(new_y).long().cuda()\n",
    "                            new_y_onehot = to_one_hot(new_y_tensor).cuda()\n",
    "\n",
    "                            print(\"debut train train1\")\n",
    "                            # Train for a few iterations\n",
    "                            model.train()\n",
    "                            for _ in range(10):\n",
    "                                optimizer.zero_grad()\n",
    "                                output, _ = model(new_x_tensor, new_y_tensor, new_y_onehot, True)\n",
    "                                loss = loss_fn(output, new_y_tensor)\n",
    "                                loss.backward()\n",
    "                                optimizer.step()\n",
    "                            model.eval()\n",
    "                            print(\"fin train train1\")\n",
    "                            th = 1\n",
    "                    if (th == 1) & (tt == win2):  # When sufficient new concept samples are collected\n",
    "                        # Retrain with new concept samples\n",
    "                        model.train()\n",
    "                        # Convert x_new and y_new to tensors for training\n",
    "                        new_x = np.array(x_new)\n",
    "                        new_y = np.array(y_new)\n",
    "                        new_x_tensor = torch.from_numpy(new_x).float().cuda()\n",
    "                        new_y_tensor = torch.from_numpy(new_y).long().cuda()\n",
    "                        new_y_onehot = to_one_hot(new_y_tensor).cuda()\n",
    "\n",
    "                        print(\"debut train train2\")\n",
    "                        # Train for a few iterations\n",
    "                        for _ in range(10):\n",
    "                            optimizer.zero_grad()\n",
    "                            output, _ = model(new_x_tensor, new_y_tensor, new_y_onehot, True)\n",
    "                            loss = loss_fn(output, new_y_tensor)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                        print(\"fin train train2\")\n",
    "                        x_new = []\n",
    "                        y_new = []\n",
    "                        d = 0  # Go back to the normal state\n",
    "                        th = 0\n",
    "                        print(\"retour etat normal\")\n",
    "\n",
    "        i = i + 1\n",
    "    score = metric.get()\n",
    "    print(f\"{a} {b} {win1} {win2} {score}\")  # Output accuracy\n",
    "    print(\"accuracy: \",m[-1])\n",
    "    print(\"F1 score: \", f1s[-1])\n",
    "    print(\"Mémoire max utilisée (MB):\", max(mems))\n",
    "    return score, t, m, dr, yt, yp\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2023a3e8cbccfce4",
   "metadata": {},
   "source": [
    "# Define OASW with a figure\n",
    "def oasw_plot(t_oasw, m_oasw, dr, t_off, m_off):\n",
    "    plt.rcParams.update({'font.size': 28})\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.clf()\n",
    "\n",
    "    # Courbe OASW+CNN\n",
    "    plt.plot(t_oasw, m_oasw, '-b', label='OASW+CNN, Accuracy: %.2f%%' % m_oasw[-1])\n",
    "\n",
    "    if t_off is not None and m_off is not None:\n",
    "        plt.plot(t_off, m_off, color='red', label='Offline CNN, Accuracy: %.2f%%' % m_off[-1])\n",
    "\n",
    "    # Points de drift\n",
    "    for i in dr:\n",
    "        plt.axvline(x=i, color='purple', linestyle='--', linewidth=2, label='Drift' if i == dr[0] else \"\")\n",
    "    for i in range(len(dr)):\n",
    "        plt.scatter(dr[i],m_oasw[dr[i]],s=200,c='r')\n",
    "\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylim(95, 101)\n",
    "    plt.xlim(0,7500)\n",
    "    plt.xlabel('Sample index', fontsize=25)\n",
    "    plt.ylabel('Accuracy (%)',fontsize=25)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cnn.pdf', format='pdf')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fa4746f1381aaef0",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    try:\n",
    "        # Clear all GPU memory before starting\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        batch_size = 200\n",
    "        train_loader = load_data(pathlist[0], pathlist[1], batchsize=batch_size)\n",
    "        test_loader = load_data(pathlist[4], pathlist[5], batchsize=1)  # batchsize=1 pour le plot\n",
    "\n",
    "        # Create model with reduced complexity if needed\n",
    "        # Reduce output_ch from 64*2 to 32*2 to save memory\n",
    "        model = convnet(input_ch=1, output_ch=64*2 , height=128 * 2, num_layers=3, num_hiden=100, num_classes=12, lr=1e-4).cuda()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-3) #weight_decay à 1e-5\n",
    "        l = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        # Early stopping parameters\n",
    "        patience = 15\n",
    "        best_accuracy = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        test_error=[]\n",
    "        val_error=[]\n",
    "\n",
    "        # Main training loop\n",
    "        max_epochs = 500\n",
    "        for epoch in range(max_epochs):\n",
    "            try:\n",
    "                # Clear memory before each epoch\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                model.set_learning_rate(0.0005)\n",
    "\n",
    "                # Print epoch information\n",
    "                print(f\"\\n{'='*20} Epoch {epoch}/{max_epochs} {'='*20}\")\n",
    "                print(f\"Learning rate: {0.0005}\")\n",
    "\n",
    "                train(train_loader, [])\n",
    "                validate(pathlist[2], pathlist[3], val_error, epoch)\n",
    "\n",
    "                # Early stopping check\n",
    "                current_error = val_error[-1] if val_error else float('inf')\n",
    "                if current_error < best_accuracy:\n",
    "                    best_accuracy = current_error\n",
    "                    patience_counter = 0\n",
    "                    # Save best model (optional)\n",
    "                    # torch.save(model.state_dict(), 'best_model.pth')\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                        break\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"CUDA out of memory\" in str(e):\n",
    "                    print(f\"OOM error in epoch {epoch}, trying to recover...\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        # Clean up resources\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import psutil",
   "id": "3b218d21db2b643e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "7e3cf2bbcd01d40",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d11c6e186084f25a",
   "metadata": {},
   "source": [
    "t_off, m_off = [], []\n",
    "f1_cnn, mem_cnn = [], []\n",
    "yt_cnn, yp_cnn = [], []\n",
    "metric = metrics.Accuracy()\n",
    "i = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        if batch_x.dim() == 3:\n",
    "            batch_x = batch_x.unsqueeze(1)\n",
    "        xi2_tensor = batch_x.cuda()\n",
    "        yi_tensor = batch_y.cuda()\n",
    "        yi_onehot = to_one_hot(yi_tensor).cuda()\n",
    "        output, _ = model(xi2_tensor, yi_tensor, yi_onehot, False)\n",
    "        _, y_pred = torch.max(output.data, 1)\n",
    "        y_pred_scalar = y_pred.cpu().item()\n",
    "        yi = batch_y.cpu().item()\n",
    "        metric.update(int(yi), y_pred_scalar)\n",
    "        yt_cnn.append(yi)\n",
    "        yp_cnn.append(y_pred_scalar)\n",
    "        t_off.append(i)\n",
    "        m_off.append(metric.get() * 100)\n",
    "        f1_cnn.append(f1_score(yt_cnn, yp_cnn, average='weighted'))\n",
    "        mem_cnn.append(psutil.Process().memory_info().rss / (1024 ** 2))\n",
    "        i += 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"CNN accuracy :\", metric.get()*100)\n",
    "print(\"CNN F1 score final :\", f1_cnn[-1]*100)\n",
    "print(\"CNN mem final : (MB)\", mem_cnn[-1])"
   ],
   "id": "b77da7cd3bd4fb95",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ad7203b1a159bf60",
   "metadata": {},
   "source": [
    "import copy\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "a_opti = 0.986\n",
    "b_opti = 0.959\n",
    "w1_opti = 100\n",
    "w2_opti = 150"
   ],
   "id": "ef734dffa5cd16f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90c57c5be88064a",
   "metadata": {},
   "source": [
    "print(\"Running OASW with CNN...\")\n",
    "model_copy = copy.deepcopy(model)\n",
    "opti_copy = torch.optim.Adam(model_copy.parameters(), lr=0.0005, weight_decay=1e-3)\n",
    "score, t_oasw, m_oasw, dr, yt, yp = oasw(model_copy, opti_copy, l, test_loader, a=a_opti, b=b_opti, win1=w1_opti, win2=w2_opti)\n",
    "#weight_decay à 1e-5 avant\n",
    "print(\"OASW with CNN completed.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "67c146c17753bcf6",
   "metadata": {},
   "source": "oasw_plot(t_oasw, m_oasw, dr, t_off, m_off)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import pyswarms as ps\n",
    "\n",
    "# Fonction objectif pour PSO\n",
    "def pso_objective(x):\n",
    "    # x est un tableau de shape (n_particles, n_params)\n",
    "    scores = []\n",
    "    for params in x:\n",
    "        a, b, win1, win2 = params\n",
    "        # Contraintes sur les bornes\n",
    "        a = np.clip(a, 0.95, 0.999)\n",
    "        b = np.clip(b, 0.95, 0.999)\n",
    "        win1 = int(np.clip(win1, 10, 2000))\n",
    "        win2 = int(np.clip(win2, 100, 4000))\n",
    "        mc = copy.deepcopy(model)\n",
    "        score, _, _, _ = oasw(mc, torch.optim.Adam(mc.parameters(), lr=0.0005, weight_decay=1e-3), l, test_loader, a=a, b=b, win1=win1, win2=win2)\n",
    "        # On veut maximiser, donc on retourne -score pour minimiser\n",
    "        scores.append(-score)\n",
    "    return np.array(scores)\n",
    "\n",
    "# Définir les bornes pour chaque hyperparamètre\n",
    "bounds = ([0.95, 0.95, 10, 100], [0.999, 0.999, 500, 3500])\n",
    "\n",
    "# Initialiser l’optimiseur PSO\n",
    "optimizer = ps.single.GlobalBestPSO(\n",
    "    n_particles=8, dimensions=4, options={'c1': 0.5, 'c2': 0.3, 'w': 0.9},\n",
    "    bounds=bounds\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "# Lancer l’optimisation\n",
    "best_cost, best_pos = optimizer.optimize(pso_objective, iters=10)\n",
    "\n",
    "print(\"Meilleurs paramètres (PSO):\", best_pos)\n",
    "print(\"Meilleure accuracy (PSO):\", -best_cost)"
   ],
   "id": "38db59783b3866a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "id": "ad6d108a38af2d27",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "229d823089a70b71",
   "metadata": {},
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les plages de recherche pour chaque paramètre\n",
    "    a = trial.suggest_float('a', 0.95, 0.999, step=0.001)\n",
    "    b = trial.suggest_float('b', 0.95, 0.999, step=0.001)\n",
    "    win1 = trial.suggest_int('win1', 10, 1500, step=50)\n",
    "    win2 = trial.suggest_int('win2', 100, 4000, step=100)\n",
    "\n",
    "    # Appeler OASW avec ces paramètres\n",
    "    mc = copy.deepcopy(model)\n",
    "    score, _, _, _,_,_ = oasw(mc, torch.optim.Adam(mc.parameters(), lr=0.0005, weight_decay=1e-3), l, test_loader,a=a, b=b, win1=win1, win2=win2)\n",
    "    return score  # Maximiser l'accuracy\n",
    "\n",
    "# Lancer l'optimisation\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)  # 30 essais, ajuste selon le temps dispo\n",
    "\n",
    "# Afficher les meilleurs paramètres trouvés\n",
    "print(\"Meilleurs paramètres :\", study.best_params)\n",
    "print(\"Meilleure accuracy :\", study.best_value)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class_names = ['Lying', 'Sitting', 'Standing', 'Walking', 'Running', 'Cycling', 'Nordic walking',\n",
    "               'Ascending stairs', 'Descending stairs', 'Vacuum cleaning', 'Ironing', 'Rope jumping']"
   ],
   "id": "91dc541c5816d256",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a9926de472125c29",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score\n",
    "\n",
    "cm = confusion_matrix(yt_cnn, yp_cnn)\n",
    "acc = accuracy_score(yt_cnn, yp_cnn)\n",
    "f1 = f1_score(yt_cnn, yp_cnn, average='weighted')\n",
    "recall = recall_score(yt_cnn, yp_cnn, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {acc*100:.2f}%')\n",
    "print(f'F1 score: {f1*100:.2f}%')\n",
    "print(f'Recall: {recall*100:.2f}%')\n",
    "\n",
    "# Affichage de la matrice de confusion\n",
    "plot_confusion(cm, class_names, title='CNN offline Confusion Matrix', save_title='CNN_offline_ConfusionMatrix.pdf')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cm_oasw = confusion_matrix(yt, yp)\n",
    "acc_oasw = accuracy_score(yt, yp)\n",
    "f1_oasw = f1_score(yt, yp, average='weighted')\n",
    "recall_oasw = recall_score(yt, yp, average='weighted')\n",
    "\n",
    "print(f'Accuracy OASW: {acc_oasw*100:.2f}%')\n",
    "print(f'F1 score OASW: {f1_oasw*100:.2f}%')\n",
    "print(f'Recall OASW: {recall_oasw*100:.2f}%')\n",
    "\n",
    "plot_confusion(cm_oasw, class_names, title='CNN OASW Confusion Matrix', save_title='CNN_OASW_ConfusionMatrix.pdf')"
   ],
   "id": "fed47345f4868d07",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
